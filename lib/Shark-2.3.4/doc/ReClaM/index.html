<!-- This comment will put IE 6, 7 and 8 in quirks mode -->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>ReClaM: Main Page</title>
<script type="text/javaScript" src="search/search.js"></script>
<link href="../css/main.css" rel="stylesheet" type="text/css"/>
</head>
<body id="type-b">

<div id="wrap">
<div id="header">
<div id="site-name">Shark Machine Learning Library</div>
<div id="poweredby">
<img style="width: 95%;" src="../images/SharkLogo.png"/>
</div>		
  
<ul id="nav">
  <li class="first"><a href="../index.html">About Shark</a></li>
  <li><a href="#">Sourceforge</a>
  
  <ul>
    <li class="first"><a href="http://shark-project.sourceforge.net">Project Summary</a></li>
    <li><a href="http://sourceforge.net/projects/shark-project/files/">Downloads</a></li>
    <li><a href="http://sourceforge.net/projects/shark-project/develop">Subversion Repository</a></li>
  </ul>
  
</li>
<li class="first"><a href="../GettingStarted.html">Getting Started</a>
<li class="first"><a href="../Tutorials.html">Tutorials</a>
<li class="first"><a href="../FAQ.html">FAQ</a>

<li class="first"><a href="#">Main Modules</a>
<ul>
  <li class="first"><a href="../ReClaM/index.html">ReClaM</a>
  <li class="first"><a href="../EALib/index.html">EALib</a>
  <li class="first"><a href="../MOO-EALib/index.html">MOO-EALib</a>
  <li class="first"><a href="../Fuzzy/index.html">Fuzzy</a>
</ul>
</li>
<li class="first"><a href="#">Tools</a>
<ul>
  <li class="first"><a href="../Mixture/index.html">Mixture</a>
  <li><a href="../Array/index.html">Array</a>
  <li><a href="../Rng/index.html">Rng</a>
  <li><a href="../LinAlg/index.html">LinAlg</a>
  <li class="last"><a href="../FileUtil/index.html">FileUtil</a>		    
</ul>
</li>
</ul>
</div>

<!--<div id="header">
<div id="site-name">Shark Machine Learning Library</div>
  <ul id="nav">
    <li><a href="../index.html"><span>Shark&nbsp;Main&nbsp;Page</span></a></li>
    <li><a href="../Array/index.html"><span>Array</span></a></li>
    <li><a href="../Rng/index.html"><span>Rng</span></a></li>
    <li><a href="../LinAlg/index.html"><span>LinAlg</span></a></li>
    <li><a href="../FileUtil/index.html"><span>FileUtil</span></a></li>
    <li><a href="../EALib/index.html"><span>EALib</span></a></li>
    <li><a href="../MOO-EALib/index.html"><span>MOO-EALib</span></a></li>
    <li class="active"><a href="../ReClaM/index.html"><span>ReClaM</span></a></li>
    <li><a href="../Fuzzy/index.html"><span>Fuzzy</span></a></li>
    <li><a href="../Mixture/index.html"><span>Mixture</span></a></li>
    <li><a href="../tutorials/index.html"><span>Tutorials</span></a></li>
    <li><a href="../faq/index.html"><span>FAQ</span></a></li>
  </ul>
 </div> -->
<!-- Generated by Doxygen 1.7.3 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li class="current"><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
    </ul>
  </div>
</div>
<div class="header">
  <div class="headertitle">
<h1>ReClaM Documentation</h1>  </div>
</div>
<div class="contents">
<div class="textblock"><p>Reference for the <b>Re</b>gression and <b>Cla</b>ssification <b>M</b>odels Toolbox.<br/>
 </p>
<h2>Overview</h2>
<p>ReClaM offers a rich set of methods for supervised learning, reaching from basic to state of the art tools. For example, the library provides simple linear models as well as different flavors of neural network and support vector machine models and training algorithms. Sophisticated model selection tools complete the collection. </p>
<p>This introduction is organized as follows: </p>
<ul>
<li>
<a href="#architecture">ReClaM Architecture</a> </li>
<li>
<a href="#linearmethods">Linear Methods</a> </li>
<li>
<a href="#neuralnetworks">Neural Networks</a> </li>
<li>
<a href="#kernelmethods">Kernel Methods</a> </li>
<li>
<a href="#supportvectormachines">Support Vector Machines</a> </li>
<li>
<a href="#modelselection">Model Selection</a> </li>
<li>
<a href="#lists">Models, Error Functions, and Optimizers</a> </li>
<li>
<a href="#further">Further features</a> </li>
</ul>
<p><a class="anchor" id="architecture"></a></p>
<h1>The ReClaM Architecture</h1>
<p><b>NOTE:</b> In the transition from Shark version 1.4.x to 1.5.x there was a major change of the ReClaM architecture. For details on design goals and technical considerations refer to the following documents:  </p>
<ul>
<li>
<a href="../ReClaM_NewArchitecture.html">Changed ReClaM Architecture</a> </li>
<li>
<a href="../ReClaM_MigrationGuide.html">ReClaM Migration Guide</a> </li>
</ul>
<p>To achieve most flexibility, the ReClaM package is build like a construction kit. The main components are:  </p>
<ul>
<li>
an adaptable (usually data processing) model </li>
<li>
an error function for model evaluation </li>
<li>
a (usually general purpose) optimizer adapting the model in order to minimize the error </li>
</ul>
<p>These components are modeled by the three ReClaM base classes  </p>
<ul>
<li>
<a class="el" href="class_model.html" title="Base class of all models.">Model</a> </li>
<li>
<a class="el" href="class_error_function.html" title="Base class of all error measures.">ErrorFunction</a> </li>
<li>
<a class="el" href="class_optimizer.html" title="Base class of all optimizers.">Optimizer</a> </li>
</ul>
<center><div align="center">
<img src="../images/ReClaMBaseClasses.png" alt="ReClaMBaseClasses.png"/>
</div>
</center> <p>The organization of the ReClaM base classes with an illustration of the information flow. </p>
<h2>A Typical ReClaM Application</h2>
<p>To give a motivation and a context for the following introduction, we will very briefly outline the general structure of a typical supervised learning scenario, that is, data driven model adaptation, within the ReClaM architecture: <br/>
 An instance of a problem specific <a class="el" href="class_model.html" title="Base class of all models.">Model</a> subclass, an <a class="el" href="class_error_function.html" title="Base class of all error measures.">ErrorFunction</a> and a suitable <a class="el" href="class_optimizer.html" title="Base class of all optimizers.">Optimizer</a> object are constructed. The training data, that is, input patterns with labels, are loaded into the system. <a class="el" href="class_model.html" title="Base class of all models.">Model</a> and optimizer are initialized. The optimization takes the form of a loop calling the optimizer. In each iteration the optimizer modifies the model parameters in order to achieve a smaller error function value than before. The error computation usually involves the model prediction on the data and the labels. This proceeding is illustrated by the following code sniplet: </p>
<div style="font-family: monospace; color: #000080; background-color:#cccccc; padding: 12px; border-width: 1px; border-color: #000080; border-style: dashed;"> &#160;// prepare data<br/>
 &#160;Array&lt;double&gt; trainingData;<br/>
 &#160;Array&lt;double&gt; trainingTargets;<br/>
 &#160;LoadOrGenerateData(trainingData, trainingTargets);<br/>
 <br/>
 &#160;// prepare basic objects<br/>
 &#160;MyModel model;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;// sub class of <a class="el" href="class_model.html" title="Base class of all models.">Model</a><br/>
 &#160;MyErrorFunction error;&#160;&#160;&#160;&#160;// sub class of <a class="el" href="class_error_function.html" title="Base class of all error measures.">ErrorFunction</a><br/>
 &#160;MyOptimizer optimizer;&#160;&#160;&#160;&#160;// sub class of <a class="el" href="class_optimizer.html" title="Base class of all optimizers.">Optimizer</a><br/>
 <br/>
 &#160;// optimize / learn<br/>
 &#160;optimizer.init(model);<br/>
 &#160;for (int i=0; i&lt;ITERATIONS; i++)<br/>
 &#160;{<br/>
 &#160;&#160;&#160;&#160;&#160;optimizer.optimize(model, error, trainingData, trainingTargets);<br/>
 &#160;}<br/>
 </div><p>The base class interfaces provide all functionality that is needed for this general scheme. Therefore, the core of all ReClaM programs looks quite similar. Now, most of the ReClaM classes are specialized subclasses implementing models, error functions or optimizers. This document will first introduce the base class interfaces and then turn to the most prominent model types. </p>
<h2>The <a class="el" href="class_model.html" title="Base class of all models.">Model</a> Base Class</h2>
<p>The <a class="el" href="class_model.html" title="Base class of all models.">Model</a> class provides an interface for an adaptable data processing model. A typical model can be thought of as a parameterized family of functions or maps. The interface provides the following functionality:  </p>
<ul>
<li>
management of a parameter vector with corresponding get and set methods: <ul>
<li>
<a class="el" href="class_model.html#a8e8eca8ae3e8e53ca0aa6a1731e1f0f6" title="Returns a specific model parameter.">Model::getParameter</a> </li>
<li>
<a class="el" href="class_model.html#a7eac39377437bd4f0f9c6368e6cbe6f4" title="Modifies a specific model parameter.">Model::setParameter</a> </li>
</ul>
</li>
<li>
processing of input data, that is, computation of model outputs <ul>
<li>
<a class="el" href="class_model.html#acc59c7d4e90e91083b86f0834eb603d9" title="Returns the model&#39;s answer #output on the stimulus #input.">Model::model</a> </li>
</ul>
</li>
<li>
computation of the derivative of the model outputs w.r.t. the model parameters <ul>
<li>
<a class="el" href="class_model.html#ac3581659f88c4fd514490e334277421a" title="Calculates the derivative of the model output with respect to the parameters.">Model::modelDerivative</a> </li>
<li>
<a class="el" href="class_model.html#a55c5dd1f4f03d41e7967d9ed765cf4c7" title="Calculates the model&#39;s general derivative.">Model::generalDerivative</a> </li>
</ul>
</li>
<li>
restriction of the feasibility of models to certain parameter ranges <ul>
<li>
<a class="el" href="class_model.html#a49d9dae98bd077daf18e7f96f2afcedc" title="check whether the parameters define a feasible model">Model::isFeasible</a> </li>
</ul>
</li>
<li>
loading and saving model parameters <ul>
<li>
Model::Load </li>
<li>
Model::Save </li>
</ul>
</li>
</ul>
<p>A model needs input patterns to perform its computations. See the <a class="el" href="class_model.html" title="Base class of all models.">Model</a> documentation for a complete list of models. </p>
<h2>The <a class="el" href="class_error_function.html" title="Base class of all error measures.">ErrorFunction</a> Base Class</h2>
<p>The <a class="el" href="class_error_function.html" title="Base class of all error measures.">ErrorFunction</a> class is a quite simple interface providing only the following functions: </p>
<ul>
<li>
computation of the error value <ul>
<li>
<a class="el" href="class_error_function.html#a116fe3002d98786b63ccc5f1a480ac14" title="Error of the model.">ErrorFunction::error</a> </li>
</ul>
</li>
<li>
computation of the derivative of the error with respect to the model parameters <ul>
<li>
<a class="el" href="class_error_function.html#a1d7581c61746f51aa96f74fc17a30d55" title="Calculates the field derivative, the derivative of the error  with respect to the model parameters...">ErrorFunction::errorDerivative</a> </li>
</ul>
</li>
</ul>
<p>An error function needs a model, input patterns and label information to perform its computations. See the <a class="el" href="class_error_function.html" title="Base class of all error measures.">ErrorFunction</a> documentation for a complete list of error functions. </p>
<h2>The <a class="el" href="class_optimizer.html" title="Base class of all optimizers.">Optimizer</a> Base Class</h2>
<p>The <a class="el" href="class_optimizer.html" title="Base class of all optimizers.">Optimizer</a> class unifies standard initialization and iterative optimization steps of all ReClaM optimizers. It provides the following interface: </p>
<ul>
<li>
standard initialization of the optimizer <ul>
<li>
<a class="el" href="class_optimizer.html#ad73a3c6a773c3ea280c59ecaee228830" title="basic initialization with default parameters">Optimizer::init</a> </li>
</ul>
</li>
<li>
iterative optimization step <ul>
<li>
<a class="el" href="class_optimizer.html#a6a8170818afe45a5516c4a4fea053f0f" title="Performes one optimization step, for example a gradient descent step or an evolution cycle...">Optimizer::optimize</a> </li>
</ul>
</li>
</ul>
<p>An optimizer needs a model, an error function as well as input patterns with labels to perform its computations. These computations result in a data driven adaptation of the model parameters in order to reduce the error evaluated on the training data. See the <a class="el" href="class_optimizer.html" title="Base class of all optimizers.">Optimizer</a> documentation for a complete list of optimizers. </p>
<p><a class="anchor" id="linearmethods"></a></p>
<h1>Linear Methods</h1>
<p>ReClaM offers five simple linear models for learning in vector spaces:  </p>
<ul>
<li>
<a class="el" href="class_linear_map.html" title="The LinearMap class represents a simple linear model  where the matrix A makes up the parameters of t...">LinearMap</a> </li>
<li>
<a class="el" href="class_linear_function.html" title="The LinearFunction class represents a simple linear function  where the vector v makes up the paramet...">LinearFunction</a> </li>
<li>
<a class="el" href="class_affine_linear_map.html" title="The LinearMap class represents a simple affine linear model  where the matrix A and the vector b make...">AffineLinearMap</a> </li>
<li>
<a class="el" href="class_affine_linear_function.html" title="The LinearFunction class represents a simple linear function  where the vector v and the offset b mak...">AffineLinearFunction</a> </li>
<li>
<a class="el" href="class_linear_classifier.html" title="The LinearClassifier class is a multi class classifier model suited for linear discriminant analysis...">LinearClassifier</a> </li>
</ul>
<p>The following standard learning methods operating on these models are available as optimizers:  </p>
<ul>
<li>
Principal Component Analysis (<a class="el" href="class_p_c_a.html" title="&quot;Principal Component Analysis&quot; class for data compression.">PCA</a>) </li>
<li>
Linear Regression </li>
<li>
Linear Discriminant Analysis </li>
</ul>
<p>These optimizers deviate from the standard ReClaM philosophy because they solve the underlying problems analytically, that is, without the need for an iterative update loop. In most cases, these algorithms can serve for preprocessing or as base line methods. </p>
<p><a class="anchor" id="neuralnetworks"></a></p>
<h1>Neural Networks</h1>
<h3>Predefined Network Models</h3>
<p>ReClaM comes with several predefined network types. These networks cover a wide variety of applications. Whenever these prototypes prove inappropriate they can hopefully serve as base classes or starting points for refined implementations. The following table lists several properties: </p>
<ul>
<li>
<b>Name</b> - The name of the class implementing the network </li>
<li>
<b>Type</b> - There are several base types of networks, possible types are <em>Feed Forward</em>, <em>Recurrent</em> and <em>Radial Basis Function</em> </li>
<li>
<b>Activation Functions</b> - Each neuron of the network has an activation function that determines how input values will be propagated through the network. The activation functions of the hidden layer and output layer neurons can be set separately. </li>
<li>
<b>Error Measures</b> - Several networks come with built in error functions, that is, they are derived from <a class="el" href="class_model.html" title="Base class of all models.">Model</a> as well as from <a class="el" href="class_error_function.html" title="Base class of all error measures.">ErrorFunction</a>. This mixing of concepts can in some cases speed up computations. The built-in error function is used for training only - it is straight forward to use a different function for evaluation. </li>
</ul>
<table  border="1" cellpadding="5" frame="box">
<tr>
<th>Name  </th><th>Type  </th><th colspan="2">Activation Functions  </th><th>Training Error Measure   </th></tr>
<tr>
<th></th><th></th><th>Hidden Neurons  </th><th>Output Neurons   </th></tr>
<tr>
<td><a class="el" href="class_f_f_net.html" title="Offers the functions to create and to work with a feed-forward network.">FFNet</a>  </td><td>Feed Forward  </td><td align="center"><div align="center">
<img src="../images/sigm1.gif" alt="sigm1.gif"/>
</div>
  </td><td align="center"><div align="center">
<img src="../images/sigm1.gif" alt="sigm1.gif"/>
</div>
  </td><td align="center">none   </td></tr>
<tr>
<td><a class="el" href="class_m_s_e_f_f_net.html" title="Offers the functions to create and to work with a feed-forward network combined with the mean squared...">MSEFFNet</a>  </td><td>Feed Forward  </td><td align="center"><div align="center">
<img src="../images/sigm1.gif" alt="sigm1.gif"/>
</div>
  </td><td align="center"><div align="center">
<img src="../images/sigm1.gif" alt="sigm1.gif"/>
</div>
  </td><td align="center">Mean Squared Error   </td></tr>
<tr>
<td><a class="el" href="class_lin_out_f_f_net.html" title="Offers the functions to create and to work with (F)eed-(F)orward (Net)works with (Lin)ear (Out)put...">LinOutFFNet</a>  </td><td>Feed Forward  </td><td align="center"><div align="center">
<img src="../images/sigm1.gif" alt="sigm1.gif"/>
</div>
  </td><td align="center"><div align="center">
<img src="../images/lin.gif" alt="lin.gif"/>
</div>
  </td><td align="center">none   </td></tr>
<tr>
<td><a class="el" href="class_lin_out_m_s_e_b_f_f_net.html" title="Offers the functions to create and to work with a a (F)eed-(F)orward (Net)works with (Lin)ear (Out)pu...">LinOutMSEBFFNet</a>  </td><td>Feed Forward  </td><td align="center"><div align="center">
<img src="../images/sigm1.gif" alt="sigm1.gif"/>
</div>
  </td><td align="center"><div align="center">
<img src="../images/lin.gif" alt="lin.gif"/>
</div>
  </td><td align="center">Mean Squared Error   </td></tr>
<tr>
<td><a class="el" href="class_tanh_net.html" title="Offers a predefined feed-forward neural network, which uses the &quot;tanh&quot; activation function ...">TanhNet</a>  </td><td>Feed Forward  </td><td align="center"><div align="center">
<img src="../images/tanh.gif" alt="tanh.gif"/>
</div>
  </td><td align="center"><div align="center">
<img src="../images/tanh.gif" alt="tanh.gif"/>
</div>
  </td><td align="center">Squared Error   </td></tr>
<tr>
<td>LinearOutputTanhNet  </td><td>Feed Forward  </td><td align="center"><div align="center">
<img src="../images/tanh.gif" alt="tanh.gif"/>
</div>
  </td><td align="center"><div align="center">
<img src="../images/lin.gif" alt="lin.gif"/>
</div>
  </td><td align="center">Squared Error   </td></tr>
<tr>
<td><a class="el" href="class_proben_net.html" title="This special network is optimal for benchmark tests with the &quot;proben 1&quot; set...">ProbenNet</a>  </td><td>Feed Forward  </td><td align="center"><div align="center">
<img src="../images/fabs.gif" alt="fabs.gif"/>
</div>
  </td><td align="center"><div align="center">
<img src="../images/lin.gif" alt="lin.gif"/>
</div>
  </td><td align="center">Mean Squared Error   </td></tr>
<tr>
<td><a class="el" href="class_proben_b_net.html" title="This special network is optimal for benchmark tests with the &quot;proben 1&quot; set...">ProbenBNet</a>  </td><td>Feed Forward  </td><td align="center"><div align="center">
<img src="../images/fabs.gif" alt="fabs.gif"/>
</div>
  </td><td align="center"><div align="center">
<img src="../images/lin.gif" alt="lin.gif"/>
</div>
  </td><td align="center">Mean Squared Error   </td></tr>
<tr>
<td><a class="el" href="class_m_s_e_r_n_net.html" title="A recurrent neural network regression model that learns with Back Propagation Through Time and assume...">MSERNNet</a>  </td><td>Recurrent  </td><td align="center"><div align="center">
<img src="../images/sigm1.gif" alt="sigm1.gif"/>
</div>
  </td><td align="center"><div align="center">
<img src="../images/sigm1.gif" alt="sigm1.gif"/>
</div>
  </td><td align="center">Mean Squared Error   </td></tr>
<tr>
<td><a class="el" href="class_r_b_f_net.html" title="Offers the functions to create and to work with radial basis function network.">RBFNet</a>  </td><td>Radial Basis Function  </td><td align="center">special  </td><td align="center">special  </td><td align="center">none   </td></tr>
<tr>
<td><a class="el" href="class_m_s_e_r_b_f_net.html" title="Offers the functions to create and to work with radial basis function networks and to train it with t...">MSERBFNet</a>  </td><td>Radial Basis Function  </td><td align="center">special  </td><td align="center">special  </td><td align="center">Mean Squared Error   </td></tr>
</table>
<h2>Connection matrix creation</h2>
<p>The methods in the file <a class="el" href="create_connection_matrix_8h.html" title="Offers methods for creating connection matrices for neural networks.">createConnectionMatrix.h</a> automatically create connection matrices for feed-forward and recurrent networks with several layers.<br/>
 Flags are offered that can be set to establish standard connections between the neurons of the network. </p>
<h2>Monitoring the training</h2>
<p>The <a class="el" href="class_early_stopping.html" title="Used for monitoring purposes, to avoid overfitting.">EarlyStopping</a> class provides measures that can be used to fight overfitting during the training process. </p>
<h2>Active Learning</h2>
<p>By means of the <a class="el" href="class_variance_estimator.html" title="Offers the methods to deal with active learning of a neural network.">VarianceEstimator</a> class it is possible to enhance the learning speed of your network by allowing it to add a new pattern to the training set at each step. This pattern is chosen in a way that it will minimize the network error. </p>
<p><a class="anchor" id="kernelmethods"></a></p>
<h1>Kernel Methods</h1>
<p>ReClaM offers a variety of kernel based learning algorithms. In the following the basic concepts and the corresponding classes are introduced. </p>
<center> <div align="center">
<img src="../images/svm.png" alt="svm.png"/>
</div>
 </center> <h2>Kernel Functions</h2>
<p>Kernel based learning algorithms rely on a positive definite kernel function taking two input patterns as arguments. The kernel function can be interpreted as first mapping the inputs into a (usually high dimensional) Hilbert space and then computing the inner product in this space. A kernel based algorithm is a learning algorithm that can be formulated in terms of inner products of pairs of input patterns. These inner products are then computed using a kernel function.  </p>
<p>The <a class="el" href="class_kernel_function.html" title="Definition of a kernel function as a ReClaM model.">KernelFunction</a> interface encapsulates this functionality. Usually a parameterized family of kernel functions is considered. Because we are interested in the optimization of its parameters, <a class="el" href="class_kernel_function.html" title="Definition of a kernel function as a ReClaM model.">KernelFunction</a> is derived from <a class="el" href="class_model.html" title="Base class of all models.">Model</a>. A kernel does not provide an output given an input. Therefore it defines a new interface: </p>
<ul>
<li>
<a class="el" href="class_kernel_function.html#a8b4c4296de7e80b66bacf3d5980e4ad3" title="Evaluates the kernel function on a const object.">KernelFunction::eval</a> </li>
<li>
<a class="el" href="class_kernel_function.html#a9c50eae47066d2472de5d468fd8f7693" title="Evaluates the kernel function and computes its derivatives w.r.t.">KernelFunction::evalDerivative</a> </li>
</ul>
<p>Of course the <a class="el" href="class_kernel_function.html" title="Definition of a kernel function as a ReClaM model.">KernelFunction</a> inherits the <a class="el" href="class_model.html" title="Base class of all models.">Model</a> members for retrieving and setting parameters and checking the feasibility of the parameters.  </p>
<p>Several standard kernel functions are available, together with combinations of kernels to new kernels: </p>
<table  border="1" cellspacing="0" cellpadding="10">
<tr>
<th>Class </th><th>Parameters </th><th>Formula </th><th>Description  </th></tr>
<tr>
<td><a class="el" href="class_linear_kernel.html" title="Linear Kernel, parameter free.">LinearKernel</a> </td><td>none </td><td><img class="formulaInl" alt="$ \langle x, z \rangle = x^T z $" src="form_5.png"/> </td><td>The linear kernel is the canonical inner product.  </td></tr>
<tr>
<td><a class="el" href="class_polynomial_kernel.html" title="Polynomial Kernel.">PolynomialKernel</a> </td><td>degree d and offset v </td><td><img class="formulaInl" alt="$ (\langle x, z \rangle + v)^d $" src="form_6.png"/> </td><td>The polynomial kernel is a polynomial of the linear kernel. This representation corresponds to a feature space of monomials of the input coordinates.  </td></tr>
<tr>
<td><a class="el" href="class_r_b_f_kernel.html" title="Definition of the RBF Gaussian kernel.">RBFKernel</a> </td><td>concentration <img class="formulaInl" alt="$ \gamma $" src="form_7.png"/> </td><td><img class="formulaInl" alt="$ \exp(-\gamma \|x-z\|^2) $" src="form_8.png"/> </td><td>The Gaussian Radial Basis Function kernel is one of the most widely used kernels for vector valued data. It is known to be universal.  </td></tr>
<tr>
<td><a class="el" href="class_diag_gauss_kernel.html" title="Guassian Kernel with independent scaling of every axis.">DiagGaussKernel</a> </td><td>diagonal inverse covariance matrix M </td><td><img class="formulaInl" alt="$ \exp(-(x-z)^T M (x-z)) $" src="form_9.png"/> </td><td>Gaussian kernel with adaptible covariance matrix. The extent of the Gaussian along each coordinate axis can be controlled.  </td></tr>
<tr>
<td><a class="el" href="class_general_gauss_kernel.html" title="General Guassian Kernel.">GeneralGaussKernel</a> </td><td>inverse covariance matrix M </td><td><img class="formulaInl" alt="$ \exp(-(x-z)^T M (x-z)) $" src="form_9.png"/> </td><td>Gaussian kernel with adaptible covariance matrix. The complete shape of the Gaussian can be controlled.  </td></tr>
<tr>
<td><a class="el" href="class_normalized_kernel.html" title="Normalized version of a kernel function.">NormalizedKernel</a> </td><td>parameters of a base kernel k </td><td><img class="formulaInl" alt="$ \frac{k(x, z)}{\sqrt{k(x, x) \cdot k(z, z)}} $" src="form_10.png"/> </td><td>The normalization of a positive definite kernel is again a valid positive definite kernel. The normalization ensures that the norm of each example in feature space is 1.  </td></tr>
<tr>
<td><a class="el" href="class_weighted_sum_kernel.html" title="Weighted sum of kernel functions.">WeightedSumKernel</a> </td><td>non-negative weights w<sub>i</sub> and parameters of base kernels k<sub>i</sub> </td><td><img class="formulaInl" alt="$ \sum_i w_i k_i(x, z) $" src="form_11.png"/> </td><td>The non-negative linear combination of positive definite kernels is a valid positive definite kernel again.  </td></tr>
</table>
<h2>Simple Algorithms</h2>
<p>The most simple kernel based algorithms available are  </p>
<ul>
<li>
<a class="el" href="class_kernel_nearest_neighbor.html" title="The kernel nearest neighbor classifier is parameter free, that is, it does not require training...">KernelNearestNeighbor</a> </li>
<li>
<a class="el" href="class_kernel_mean_classifier.html" title="The kernel mean classifier is parameter free, that is, it does not require training.">KernelMeanClassifier</a> </li>
</ul>
<p>Their standard vector space versions can be obtained by simply using the linear kernel. These algorithms can serve as a baseline for the evaluation of more advanced methods. A special aspect of these algorithms is that they do not have any hyperparameters and do not need any special optimization. That is, once the training data are provided the models can be used immediately. </p>
<p><a class="anchor" id="supportvectormachines"></a></p>
<h1>Support Vector Machines</h1>
<p>Support Vector Machines (SVMs) are the most prominent kernel based learning algorithms. The basic ideas of support vector machines are as follows: </p>
<ul>
<li>
The input patterns are mapped into a Hilbert space using a kernel function. </li>
<li>
In the Hilbert space patterns of different classes are linearly separated by a hyperplane with maximal margin. </li>
<li>
To avoid overfitting only classification functions with a limited norm are considered. </li>
</ul>
<h2><a class="el" href="class_s_v_m.html" title="Support Vector Machine (SVM) as a ReClaM Model.">SVM</a> <a class="el" href="class_model.html" title="Base class of all models.">Model</a></h2>
<p>Support vector machines usually result in a sparse affine sum of the form </p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ f(x) = \sum_{i \in S} \alpha_i k(x_i, x) + b \]" src="form_12.png"/>
</p>
<p> where the support vector index set S is a subset of the training data indices. In the case of classification with more than two classes the expansion becomes </p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ f(x) = \sum_{i \in S} \sum_c \alpha_{i,c} k(x_i, x) y_c + b \]" src="form_13.png"/>
</p>
<p>  where c sums over the classes and <img class="formulaInl" alt="$ y_i $" src="form_14.png"/> are class prototype vectors.</p>
<p>In ReClaM, a support vector machine is a <a class="el" href="class_model.html" title="Base class of all models.">Model</a> holding the coefficients <img class="formulaInl" alt="$ \alpha_i $" src="form_15.png"/> and <img class="formulaInl" alt="$ b $" src="form_16.png"/>. Its prediction can be restricted to the sign of the computed value, which is the standard form for binary classification problems. For multi-class problems, the output vector can analogously be transformed into an integer class label. In contrast to other ReClaM models, support vector machines output only one value per input pattern. They can be viewed as affine linear functions mapping from the span of the support vectors in feature space to the real line (or to <img class="formulaInl" alt="$R^n$" src="form_17.png"/>). </p>
<h2><a class="el" href="class_s_v_m.html" title="Support Vector Machine (SVM) as a ReClaM Model.">SVM</a> Training</h2>
<p>Training an <a class="el" href="class_s_v_m.html" title="Support Vector Machine (SVM) as a ReClaM Model.">SVM</a> involves several so called hyperparameters. These are encapsulated by the <a class="el" href="class_meta_s_v_m.html" title="Base class of all meta models for SVM training.">MetaSVM</a> class and its descendants: </p>
<table  border="1" cellspacing="0" cellpadding="10">
<tr>
<th>Class </th><th>Parameters </th><th>Problem Type </th><th>Description  </th></tr>
<tr>
<td><a class="el" href="class_c___s_v_m.html" title="Meta Model for SVM training.">C_SVM</a> </td><td>regularization parameter C </td><td>binary classification </td><td>C-SVM formulation using either 1-norm or 2-norm slack penalty. This is the most usual way to train a support vector machine.  </td></tr>
<tr>
<td><a class="el" href="class_epsilon___s_v_m.html" title="Meta Model for SVM training.">Epsilon_SVM</a> </td><td>regularization parameter C, accuracy parameter <img class="formulaInl" alt="$ \varepsilon $" src="form_18.png"/> </td><td>regression </td><td>Standard <a class="el" href="class_s_v_m.html" title="Support Vector Machine (SVM) as a ReClaM Model.">SVM</a> for regression  </td></tr>
<tr>
<td><a class="el" href="class_regularization_network.html" title="Meta Model for SVM training.">RegularizationNetwork</a> </td><td>regularization parameter <img class="formulaInl" alt="$ \gamma $" src="form_7.png"/> </td><td>regression </td><td>regularized linear regression in feature space  </td></tr>
<tr>
<td><a class="el" href="class_gaussian_process.html" title="Gaussian Process.">GaussianProcess</a> </td><td>none </td><td>regression </td><td>Bayesian inference is used to estimate the regularization and kernel parameters.  </td></tr>
<tr>
<td><a class="el" href="class_one_class_s_v_m.html" title="Meta Model for SVM training.">OneClassSVM</a> </td><td>regularization parameter <img class="formulaInl" alt="$ \nu $" src="form_19.png"/> </td><td>support / density quantile estimation </td><td>Estimation of an area of high density  </td></tr>
<tr>
<td><a class="el" href="class_all_in_one_mc_s_v_m.html" title="Meta Model for SVM training.">AllInOneMcSVM</a> </td><td>regularization parameter C </td><td>multi-category classification </td><td>Covers the methods by Weston and Watkins and by Wahba (without bias)  </td></tr>
<tr>
<td><a class="el" href="class_crammer_singer_mc_s_v_m.html" title="Meta Model for SVM training.">CrammerSingerMcSVM</a> </td><td>regularization parameter <img class="formulaInl" alt="$ \beta $" src="form_20.png"/> </td><td>multi-category classification </td><td>Multi-Class <a class="el" href="class_s_v_m.html" title="Support Vector Machine (SVM) as a ReClaM Model.">SVM</a> by Crammer and Singer (without bias)  </td></tr>
<tr>
<td><a class="el" href="class_o_v_a_mc_s_v_m.html" title="Meta Model for SVM training.">OVAMcSVM</a> </td><td>regularization parameter C </td><td>multi-category classification </td><td>A simple one-versus-all machine (without bias)  </td></tr>
<tr>
<td>BinaryCostMcSVM </td><td>regularization parameter C </td><td>multi-category classification </td><td>Multi-class classification at the training cost of a binary machine (without bias)  </td></tr>
</table>
<p>For the adaptation of the hyperparameters of the <a class="el" href="class_c___s_v_m.html" title="Meta Model for SVM training.">C_SVM</a> or the <a class="el" href="class_epsilon___s_v_m.html" title="Meta Model for SVM training.">Epsilon_SVM</a>, please refer to the model selection section below. </p>
<p>SVMs are trained solving a special quadratic program. For this purpose ReClaM comes with a various efficient solvers for the particular types of optimization problems. The quadratic programming is hidden from the user of the library. An <a class="el" href="class_s_v_m.html" title="Support Vector Machine (SVM) as a ReClaM Model.">SVM</a> is trained using the special <a class="el" href="class_s_v_m___optimizer.html" title="Optimizer for SVM training by quadratic programming.">SVM_Optimizer</a> class. This class constructs a QuadraticProgram object, solves the corresponding problem and copies the solution into the <a class="el" href="class_s_v_m.html" title="Support Vector Machine (SVM) as a ReClaM Model.">SVM</a> parameter vector. The optimization involves a special quadratic objective function. For efficiency reasons it is not desirable to define this objective function as an <a class="el" href="class_error_function.html" title="Base class of all error measures.">ErrorFunction</a> derived class and use a standard purpose gradient based optimizer. Because of this special situation the <a class="el" href="class_s_v_m___optimizer.html" title="Optimizer for SVM training by quadratic programming.">SVM_Optimizer</a> can be called without an error function parameter. When called with an error function through the standard <a class="el" href="class_optimizer.html" title="Base class of all optimizers.">Optimizer</a> interface the error function is ignored. </p>
<p><a class="anchor" id="modelselection"></a></p>
<h1><a class="el" href="class_model.html" title="Base class of all models.">Model</a> Selection</h1>
<p>The process of model selection is the choice of a model from a family of candidate models. This may correspond to the selection of the hyperparameters of a <a class="el" href="class_meta_s_v_m.html" title="Base class of all meta models for SVM training.">MetaSVM</a> subclass. In this case the model family is parameterized by the <a class="el" href="class_meta_s_v_m.html" title="Base class of all meta models for SVM training.">MetaSVM</a> parameter vector. </p>
<h2>General <a class="el" href="class_model.html" title="Base class of all models.">Model</a> Selection</h2>
<p>A model selection problem is usually solved by an outer optimization loop investigating a model family and an inner optimization loop finding the best model within this family. The fitness of a model family is simply the fitness of its best element.  </p>
<p>In general, model selection can be carried out on complicated spaces like the possible topologies of a neural network. These problems require specialized evolutionary algorithms not provided by the ReClaM library. However, in the case of support vector machines the model selection problem can usually be reduced to real valued parameter adaptation, to which ReClaM is perfectly suited. Therefore most model selection support is available for this model type. Only the cross validation method is applicable to general model selection problems. </p>
<h2><a class="el" href="class_model.html" title="Base class of all models.">Model</a> Selection for Support Vector Machines</h2>
<p>The model of a support vector machine is made up by the kernel function and the complexity control. The kernel parameters and the <a class="el" href="class_s_v_m.html" title="Support Vector Machine (SVM) as a ReClaM Model.">SVM</a> training parameters are to be chosen. These parameters are captured by the <a class="el" href="class_meta_s_v_m.html" title="Base class of all meta models for SVM training.">MetaSVM</a> class and its subclasses.  </p>
<p>Several objective functions have been proposed for model selection for support vector machines. Besides the usual cross validation measures ReClaM provides the following <a class="el" href="class_error_function.html" title="Base class of all error measures.">ErrorFunction</a> subclasses:  </p>
<ul>
<li>
<a class="el" href="class_radius_margin.html" title="Squared Radius-Margin-Quotient.">RadiusMargin</a> - radius margin quotient </li>
<li>
<a class="el" href="class_negative_k_t_a.html" title="Implementation of the negative Kernel Target Alignment (KTA) as proposed by Nello Cristianini...">NegativeKTA</a> - (negative) kernel target alignment </li>
<li>
<a class="el" href="class_negative_b_k_t_a.html" title="Balanced version of the NegativeKTA.">NegativeBKTA</a> - (negative) balanced kernel target alignment </li>
<li>
<a class="el" href="class_negative_polarization.html" title="Implementation of the negative Kernel Polarization Measure, that is, Kernel Target Alignment without ...">NegativePolarization</a> - (negative) kernel polarization </li>
<li>
<a class="el" href="class_l_o_o.html" title="Leave One Out (LOO) Error for Support Vector Machines.">LOO</a> - leave one out error (specially tuned for SVMs) </li>
<li>
<a class="el" href="class_span_bound.html" title="SpanBound for the 2-norm SVM.">SpanBound</a> - span bound computation </li>
</ul>
<p><a class="anchor" id="examples"></a> </p>
<h2>Example Programs</h2>
<p>To give you a better idea, how using the several components of ReClaM to build your own networks, you can take a look at some commented source codes of <a href="examples.html">example programs</a> for different types of predefined networks. To run these programs please go to the examples directory of ReClaM.  </p>
<p>Please have a look at our introductory tutorials on using multi-layer perceptron networks, <a href="../tutorials/MLP/index.html">part 1</a> and <a href="../tutorials/MLP2/index.html">part 2</a>. </p>
<p><a class="anchor" id="lists"></a></p>
<h1>Models, Error Functions, and Optimizers</h1>
<h2>Models</h2>
<p>The following table lists the most prominent ReClaM model types: </p>
<table  border="1" cellspacing="0" cellpadding="10">
<tr>
<th>Name </th><th>Inputs </th><th>Outputs </th><th>Description  </th></tr>
<tr>
<td><a class="el" href="class_linear_map.html" title="The LinearMap class represents a simple linear model  where the matrix A makes up the parameters of t...">LinearMap</a> </td><td><img class="formulaInl" alt="$ R^n $" src="form_21.png"/> </td><td><img class="formulaInl" alt="$ R^m $" src="form_22.png"/> </td><td>Linear mapping <img class="formulaInl" alt="$ x \mapsto y = A x $" src="form_23.png"/>. An affine version is available, too.  </td></tr>
<tr>
<td><a class="el" href="class_linear_function.html" title="The LinearFunction class represents a simple linear function  where the vector v makes up the paramet...">LinearFunction</a> </td><td><img class="formulaInl" alt="$ R^n $" src="form_21.png"/> </td><td><img class="formulaInl" alt="$ R $" src="form_24.png"/> </td><td>Special case of a linear map with one output dimension. An affine version is available, too.  </td></tr>
<tr>
<td><a class="el" href="class_component_wise_model.html" title="The ComponentWiseModel encapsulates the component wise application of a base model.">ComponentWiseModel</a> </td><td><img class="formulaInl" alt="$ X^k $" src="form_25.png"/> </td><td><img class="formulaInl" alt="$ Y^k $" src="form_26.png"/> </td><td>Apply a base model <img class="formulaInl" alt="$ f : X \rightarrow Y $" src="form_27.png"/> <img class="formulaInl" alt="$k$" src="form_28.png"/>-fold and component wise, making up a model <img class="formulaInl" alt="$ F : X^k \rightarrow Y^k $" src="form_29.png"/>.  </td></tr>
<tr>
<td>ConcatenatedModel </td><td><img class="formulaInl" alt="$ X $" src="form_30.png"/> </td><td><img class="formulaInl" alt="$ Y $" src="form_31.png"/> </td><td>Apply a chain <img class="formulaInl" alt="$ f_1 \circ f_2 \circ \dots \circ f_k $" src="form_32.png"/> of models. The output space of model <img class="formulaInl" alt="$ f_i $" src="form_33.png"/> and the input space of model <img class="formulaInl" alt="$ f_{i+1} $" src="form_34.png"/> must match.  </td></tr>
<tr>
<td><a class="el" href="class_f_f_net.html" title="Offers the functions to create and to work with a feed-forward network.">FFNet</a> </td><td><img class="formulaInl" alt="$ R^n $" src="form_21.png"/> </td><td><img class="formulaInl" alt="$ R^m $" src="form_22.png"/> </td><td>feed forward neural network  </td></tr>
<tr>
<td><a class="el" href="class_r_b_f_net.html" title="Offers the functions to create and to work with radial basis function network.">RBFNet</a> </td><td><img class="formulaInl" alt="$ R^n $" src="form_21.png"/> </td><td><img class="formulaInl" alt="$ R $" src="form_24.png"/> </td><td>radial basis function network  </td></tr>
<tr>
<td><a class="el" href="class_m_s_e_r_n_net.html" title="A recurrent neural network regression model that learns with Back Propagation Through Time and assume...">MSERNNet</a> </td><td><img class="formulaInl" alt="$ R^n $" src="form_21.png"/> </td><td><img class="formulaInl" alt="$ R^m $" src="form_22.png"/> </td><td>recurrent neural network with built-in mean squared error computation  </td></tr>
<tr>
<td><a class="el" href="class_kernel_function.html" title="Definition of a kernel function as a ReClaM model.">KernelFunction</a> </td><td><img class="formulaInl" alt="$ X \times X $" src="form_35.png"/> </td><td><img class="formulaInl" alt="$ R $" src="form_24.png"/> </td><td>Please refer to the <a href="#kernelmethods">Kernel Methods</a> documentation.  </td></tr>
<tr>
<td><a class="el" href="class_kernel_mean_classifier.html" title="The kernel mean classifier is parameter free, that is, it does not require training.">KernelMeanClassifier</a> </td><td><img class="formulaInl" alt="$ X $" src="form_30.png"/> </td><td><img class="formulaInl" alt="$ R $" src="form_24.png"/> </td><td>Simple mean classifier in a kernel defined feature space  </td></tr>
<tr>
<td><a class="el" href="class_kernel_nearest_neighbor.html" title="The kernel nearest neighbor classifier is parameter free, that is, it does not require training...">KernelNearestNeighbor</a> </td><td><img class="formulaInl" alt="$ X $" src="form_30.png"/> </td><td><img class="formulaInl" alt="$ \{-1, +1\} $" src="form_36.png"/> </td><td>Simple k-nearest-neighbor classifier in a kernel defined feature space  </td></tr>
<tr>
<td><a class="el" href="class_s_v_m.html" title="Support Vector Machine (SVM) as a ReClaM Model.">SVM</a> </td><td><img class="formulaInl" alt="$ X $" src="form_30.png"/> </td><td><img class="formulaInl" alt="$ \{-1, +1\} $" src="form_36.png"/> or <img class="formulaInl" alt="$ R $" src="form_24.png"/> </td><td>Support Vector Machine model, that is, affine linear function in a kernel defined feature space.  </td></tr>
<tr>
<td><a class="el" href="class_multi_class_s_v_m.html" title="Multi Class Support Vector Machine Model.">MultiClassSVM</a> </td><td><img class="formulaInl" alt="$ X $" src="form_30.png"/> </td><td><img class="formulaInl" alt="$ \{0, \dots, \#classes-1\} $" src="form_37.png"/> or <img class="formulaInl" alt="$ R^n $" src="form_21.png"/> </td><td>Support Vector Machine model, that is, affine linear function in a kernel defined feature space.  </td></tr>
<tr>
<td><a class="el" href="class_svm_approximation_model.html" title="Approximation of Support Vector Machines (SVMs)">SvmApproximationModel</a> </td><td></td><td></td><td>Used internally. Please refer to the <a class="el" href="class_svm_approximation.html" title="Approximation of Support Vector Machines (SVMs)">SvmApproximation</a> class for documentation.  </td></tr>
<tr>
<td><a class="el" href="class_meta_s_v_m.html" title="Base class of all meta models for SVM training.">MetaSVM</a> </td><td>--- </td><td>--- </td><td>Base class of all <a class="el" href="class_s_v_m.html" title="Support Vector Machine (SVM) as a ReClaM Model.">SVM</a> training schemes. See the <a class="el" href="class_meta_s_v_m.html" title="Base class of all meta models for SVM training.">MetaSVM</a> documentation or the section <a href="#supportvectormachines">Support Vector Machines</a> for details.  </td></tr>
<tr>
<td><a class="el" href="class_sigmoid_model.html" title="Standard sigmoid function with two parameters.">SigmoidModel</a> </td><td>R </td><td>(0, 1) </td><td>sigmoidal function <img class="formulaInl" alt="$ f(x) = \frac{1}{1 + \exp(ax+b)} $" src="form_38.png"/>  </td></tr>
</table>
<p><br/>
<br/>
<hr/>
<br/>
</p>
<h2>Error Functions</h2>
<p>This table gives an overview over the error functions available: </p>
<table  border="1" cellspacing="0" cellpadding="10">
<tr>
<th>Class </th><th>Differentiable </th><th>Restrictions </th><th>Description  </th></tr>
<tr>
<td><a class="el" href="class_squared_error.html" title="Calculates the sum-of-squares error.">SquaredError</a> </td><td>yes </td><td>none </td><td>squared error between model output and target  </td></tr>
<tr>
<td><a class="el" href="class_mean_squared_error.html" title="Calculates the mean squared error.">MeanSquaredError</a> / <a class="el" href="class_d_f___mean_squared_error.html" title="Mean Squared Error, using the GeneralDerivative interface.">DF_MeanSquaredError</a> </td><td>yes </td><td>none </td><td>mean squared error between model output and target  </td></tr>
<tr>
<td><a class="el" href="class_cross_entropy.html" title="Error measure for classication tasks that can be used as the objective function for training...">CrossEntropy</a> / <a class="el" href="class_d_f___cross_entropy.html" title="Error measure for classication tasks that can be used as the objective function for training...">DF_CrossEntropy</a> </td><td>yes </td><td>unit interval model outputs </td><td>cross entropy measure for neural network classifier training  </td></tr>
<tr>
<td><a class="el" href="class_cross_entropy_independent.html" title="Error measure for classification tasks of non exclusive attributes that can be used for model trainin...">CrossEntropyIndependent</a> / <a class="el" href="class_d_f___cross_entropy_independent.html" title="Error measure for classification tasks of non exclusive attributes that can be used for model trainin...">DF_CrossEntropyIndependent</a> </td><td>yes </td><td>unit interval model outputs </td><td>cross entropy measure for neural network classifier training with non exclusive attributes  </td></tr>
<tr>
<td><a class="el" href="class_classification_error.html" title="The ClassificationError class returns the number of classification errors.">ClassificationError</a> </td><td>no </td><td>Classifiers </td><td>fraction of misclassified patterns  </td></tr>
<tr>
<td><a class="el" href="class_balanced_classification_error.html" title="The ClassificationError class returns the number of classification errors, rescaled by the class magn...">BalancedClassificationError</a> </td><td>no </td><td>Classifiers </td><td>mean fraction of misclassified patterns per class  </td></tr>
<tr>
<td><a class="el" href="class_error_percentage.html" title="Calculates the error percentage based on the mean squared error.">ErrorPercentage</a> </td><td>no </td><td>none </td><td>error percentage based on the mean squared error  </td></tr>
<tr>
<td><a class="el" href="class_radius_margin.html" title="Squared Radius-Margin-Quotient.">RadiusMargin</a> </td><td>yes </td><td><a class="el" href="class_kernel_function.html" title="Definition of a kernel function as a ReClaM model.">KernelFunction</a> derived classes and <a class="el" href="class_c___s_v_m.html" title="Meta Model for SVM training.">C_SVM</a> with 2-norm slack penalty </td><td>squared radius margin quotient for <a class="el" href="class_s_v_m.html" title="Support Vector Machine (SVM) as a ReClaM Model.">SVM</a> model selection  </td></tr>
<tr>
<td><a class="el" href="class_negative_k_t_a.html" title="Implementation of the negative Kernel Target Alignment (KTA) as proposed by Nello Cristianini...">NegativeKTA</a> </td><td>yes </td><td><a class="el" href="class_kernel_function.html" title="Definition of a kernel function as a ReClaM model.">KernelFunction</a> derived classes and <a class="el" href="class_c___s_v_m.html" title="Meta Model for SVM training.">C_SVM</a> with 2-norm slack penalty </td><td>negative kernel target alignment  </td></tr>
<tr>
<td><a class="el" href="class_negative_b_k_t_a.html" title="Balanced version of the NegativeKTA.">NegativeBKTA</a> </td><td>yes </td><td><a class="el" href="class_kernel_function.html" title="Definition of a kernel function as a ReClaM model.">KernelFunction</a> derived classes and <a class="el" href="class_c___s_v_m.html" title="Meta Model for SVM training.">C_SVM</a> with 2-norm slack penalty </td><td>negative balanced kernel target alignment  </td></tr>
<tr>
<td><a class="el" href="class_negative_polarization.html" title="Implementation of the negative Kernel Polarization Measure, that is, Kernel Target Alignment without ...">NegativePolarization</a> </td><td>yes </td><td><a class="el" href="class_kernel_function.html" title="Definition of a kernel function as a ReClaM model.">KernelFunction</a> derived classes and <a class="el" href="class_c___s_v_m.html" title="Meta Model for SVM training.">C_SVM</a> with 2-norm slack penalty </td><td>negative kernel polarization measure  </td></tr>
<tr>
<td><a class="el" href="class_span_bound.html" title="SpanBound for the 2-norm SVM.">SpanBound</a> </td><td>no </td><td><a class="el" href="class_s_v_m.html" title="Support Vector Machine (SVM) as a ReClaM Model.">SVM</a> classifier </td><td>span bound for a support vector machine classifier model selection  </td></tr>
<tr>
<td><a class="el" href="class_l_o_o.html" title="Leave One Out (LOO) Error for Support Vector Machines.">LOO</a> </td><td>no </td><td><a class="el" href="class_s_v_m.html" title="Support Vector Machine (SVM) as a ReClaM Model.">SVM</a> classifier </td><td>leave one out error especially tuned towards support vector machines  </td></tr>
<tr>
<td><a class="el" href="class_inverse_class_separability.html" title="Inverse of the Class Separability Measure J by Huilin Xiong and M.">InverseClassSeparability</a> </td><td>yes </td><td><a class="el" href="class_kernel_function.html" title="Definition of a kernel function as a ReClaM model.">KernelFunction</a> derived classes </td><td>inverse of the class separability measure termed J, by Huilin Xiong and M. N. S. Swamy  </td></tr>
</table>
<p><br/>
<br/>
<hr/>
<br/>
</p>
<h2>Optimizers</h2>
<p>Among others, here come the most used optimizers: </p>
<table  border="1" cellspacing="0" cellpadding="10">
<tr>
<th>Class </th><th>Parameters </th><th>Gradient-Based? </th><th>Description  </th></tr>
<tr>
<td><a class="el" href="class_i_rprop_plus.html" title="This class offers methods for the usage of the improved Resilient-Backpropagation-algorithm with weig...">IRpropPlus</a> </td><td>initial step sizes, increase and decrease factors, minimal and maximal step sizes </td><td>yes </td><td>Improved resilent backpropagation with weight backtracking. Other variants without improvement or backtracking are available.  </td></tr>
<tr>
<td><a class="el" href="class_c_g.html" title="Offers methods to use the Conjugate Gradients algorithm for the optimization of models.">CG</a> </td><td>line search type and several tuning parameters with default values </td><td>yes </td><td>Conjugate Gradient method  </td></tr>
<tr>
<td><a class="el" href="class_b_f_g_s.html" title="Offers methods to use the Broyden-Fletcher-Goldfarb-Shanno algorithm for the optimization of models...">BFGS</a> </td><td>line search type and several tuning parameters with default values </td><td>yes </td><td>Broyden-Fletcher-Goldfarb-Shanno (<a class="el" href="class_b_f_g_s.html" title="Offers methods to use the Broyden-Fletcher-Goldfarb-Shanno algorithm for the optimization of models...">BFGS</a>) algorithm  </td></tr>
<tr>
<td><a class="el" href="class_c_m_a_optimizer.html" title="The CMA-ES as a ReClaM Optimizer.">CMAOptimizer</a> </td><td>CMA type, initial standard deviations </td><td>no </td><td>Covariance Matrix Adaptation Evolution Strategy. This is a wrapper class for the EALib CMA implementation. The CMA is the first choice optimizer if the derivative is not available or lots of undesirable local minima are a concern.  </td></tr>
<tr>
<td><a class="el" href="class_grid_search.html" title="Optimize by trying out a grid of configurations.">GridSearch</a> </td><td>grid definition </td><td>no </td><td>Grid search is the most basic optimization method available. Several variants add flexibility and/or improve efficiency.  </td></tr>
</table>
<p><a class="anchor" id="further"></a></p>
<h1>Further features</h1>
<p>ReClaM offers a bunch of other models, error functions, and optimizers, as well as tools, e.g. for data handling, unified exception handling, and even more. Check out the <a href="annotated.html">class</a> or <a href="files.html">file</a> lists for a full overview.</p>
<p>For example, have a look at the tools defined in </p>
<ul>
<li>
<a class="el" href="create_connection_matrix_8h.html" title="Offers methods for creating connection matrices for neural networks.">createConnectionMatrix.h</a> </li>
<li>
<a class="el" href="_dataset_8h.html" title="Functions for loading ReClaM datasets.">Dataset.h</a>, <a class="el" href="_artificial_distributions_8h.html" title="Artificial benchmark data.">ArtificialDistributions.h</a> </li>
<li>
<a class="el" href="_cross_validation_8h.html" title="Cross Validation.">CrossValidation.h</a> </li>
<li>
RException.h </li>
<li>
<a class="el" href="_svm_approximation_8h.html" title="Approximation of Support Vector Machines (SVMs)">SvmApproximation.h</a> </li>
</ul>
<p>Maybe the fastest way to get a grip on ReClaM is to have a look at the <a href="examples.html">examples</a>. </p>
</div></div>
</div>
</div>
</body></html>
